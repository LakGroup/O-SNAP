function trained_classifier = run_SNAP_classification(data,options)
arguments
    data table
    options.test_data = []
    options.pca_result struct = []
    options.vars_selected cell = {}
    options.model_type string = 'tree_fine'
    options.k double = 5;
    options.verbose logical = false
end
valid_model_types = ["tree_fine","tree_medium","tree_coarse",...
    "discriminant_linear","discriminant_quadratic",...
    "logistic_regression_binary_glm",...
    "logistic_regression_efficient","svm_efficient_linear",...
    "naive_bayes_gaussian","naive_bayes_kernel",...
    "svm_linear","svm_quadratic",...
    "svm_gaussian_fine","svm_gaussian_medium","svm_gaussian_coarse",...
    "knn_fine","knn_medium","knn_coarse","knn_cosine","knn_cubic","knn_weighted",...
    "ensemble_boosted_trees","ensemble_bagged_trees","ensemble_subspace_discriminant","ensemble_rus_boosted_trees",...
    "NN_narrow","NN_medium","NN_wide","NN_bilayered","NN_trilayered",...
    "kernel_svm","kernel_logistic_regression"];
assert(ismember(options.model_type,valid_model_types),...
    'SNAP:invalid_classification_model',...
    'Invalid model type: %s, choose from following valid models:\ntree_fine,tree_medium,tree_coarse,\ndiscriminant_linear,discriminant_quadratic,\nlogistic_regression_binary_glm,\nlogistic_regression_efficient,svm_efficient_linear,\nnaive_bayes_gaussian,naive_bayes_kernel,\nsvm_linear,svm_quadratic,\nsvm_gaussian_fine,svm_gaussian_medium,svm_gaussian_coarse,\nknn_fine,knn_medium,knn_coarse,knn_cosine,knn_cubic,knn_weighted,\nensemble_boosted_trees,ensemble_bagged_trees,ensemble_subspace_discriminant,ensemble_rus_boosted_trees,\nNN_narrow,NN_medium,NN_wide,NN_bilayered,NN_trilayered,\nkernel_svm,kernel_logistic_regression',options.model_type);

trained_classifier = struct();

data = preprocess_SNAP_table(data,'normalize',false);

%% Get class names (groups) and predictors (preprocessed feature data)
% Also adds information depending on if variable selection or PCA info is
% passed to the function. Variable selection information from a PCA takes
% precedence over ones passed into the function, if they are inconsistent.
if ~isempty(options.pca_result)
    [train_data,response] = preprocess_SNAP_table(data,'numeric_only',true);
    pca_result = options.pca_result;
    predictors = pca_result.pca_transformation_fcn(train_data);
    trained_classifier.PCACenters = pca_result.pca_centers;
    trained_classifier.PCACoefficients = pca_result.pca_coefficients;
    trained_classifier.RequiredVariables = pca_result.vars_selected;
elseif ~isempty(options.vars_selected)
    [predictors,response] = preprocess_SNAP_table(data(:,['group' options.vars_selected]),'numeric_only',true);
    trained_classifier.RequiredVariables = options.vars_selected;
else
    [predictors,response] = preprocess_SNAP_table(data,'numeric_only',true);
    trained_classifier.RequiredVariables = predictors.Properties.VariableNames;
end

%% Train a classifier
% This code specifies all the classifier options and trains the classifier.
[classification_model, predict_fcn] = get_model(options.model_type,predictors,response);
% Add additional fields to the result struct
trained_classifier.ModelType = options.model_type;
trained_classifier.ClassificationModel = classification_model;
trained_classifier.About = 'This struct is a trained model exported from Classification Learner R2024b.';
trained_classifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  [yfit,scores] = c.predictFcn(T) \nreplace ''c'' with the name of the variable that is this struct, e.g. ''trainedModel''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');
% Create the result struct with predict function
if ~isempty(options.pca_result)
    trained_classifier.predictFcn = @(x) predict_fcn(pca_result.pca_transformation_fcn(x));
else
    predictor_extraction_fcn = @(t) normalize(t(~any(ismissing(t),2), trained_classifier.RequiredVariables));
    trained_classifier.predictFcn = @(x) predict_fcn(predictor_extraction_fcn(x));
end

%% Calculate accuracy on test data
if ~isempty(options.test_data)
    [test_data, trained_classifier.TestResponse] = preprocess_SNAP_table(options.test_data,'numeric_only',true);
    [test_predictions, test_scores] = trained_classifier.predictFcn(test_data);
    trained_classifier.TestScores = test_scores;
    trained_classifier.TestPredictions = strtrim(test_predictions);
    trained_classifier.TestResponse = strtrim(trained_classifier.TestResponse);
    correct_predictions = strcmp(...
        trained_classifier.TestPredictions,...
        trained_classifier.TestResponse);
    trained_classifier.TestAccuracy = sum(correct_predictions)/numel(correct_predictions);
    groups = unique(trained_classifier.TestResponse);
    trained_classifier.PerformanceCurve = cell(numel(groups),1);
    for i=1:numel(groups)
        trained_classifier.PerformanceCurve{i}.group = groups(i);
        [trained_classifier.PerformanceCurve{i}.x,trained_classifier.PerformanceCurve{i}.y,trained_classifier.PerformanceCurve{i}.threshold,trained_classifier.PerformanceCurve{i}.AUC] = perfcurve(trained_classifier.TestResponse,trained_classifier.TestScores(:,i),groups(i));
    end
    % Show results
    if options.verbose
        fprintf('Model: %s\n  Acc: %5.2f\n',options.model_type,trained_classifier.TestAccuracy*100)
    end

%% Calculate validation accuracy if no test data is given
else
    % Perform cross-validation
    options.k = 5;
    cvp = cvpartition(response, 'KFold', options.k);
    % Initialize the predictions to the proper sizes
    trained_classifier.ValidationPredictions = response;
    n_obs = size(predictors,1);
    n_groups = numel(unique(response));
    trained_classifier.ValidationScores = NaN(n_obs,n_groups);
    for k = 1:options.k
        % split train from test
        train_k = data(cvp.training(k),:);
        % PCA on train data
        if ~isempty(options.pca_result)
            [train_k,response_k] = preprocess_SNAP_table(train_k,'numeric_only',true);
            pca_result_k = run_SNAP_PCA(train_k,...
                                "vars_sel",options.vars_selected,...
                                "num_components_explained",options.pca_result.num_comps_to_keep);
            predictors_k = pca_result_k.pca_transformation_fcn(train_k);
        elseif ~isempty(options.vars_selected)
            [predictors_k,response_k] = preprocess_SNAP_table(train_k(:,['group' options.vars_selected]),'numeric_only',true);
        else
            [predictors_k,response_k] = preprocess_SNAP_table(train_k,'numeric_only',true);
        end
        % Train classifier
        [~, predict_fcn_k] = get_model(options.model_type,predictors_k,response_k);
        if ~isempty(options.pca_result)
            predictFcn_k = @(x) predict_fcn_k(pca_result_k.pca_transformation_fcn(x));
        else
            predictor_extraction_fcn_k = @(t) normalize(t(~any(ismissing(t),2), trained_classifier.RequiredVariables));
            predictFcn_k = @(x) predict_fcn_k(predictor_extraction_fcn_k(x));
        end
        % Compute validation predictions
        test_k = data(cvp.test(k),:);
        [predictors_k, ~] = preprocess_SNAP_table(test_k,'numeric_only',true);
        [predictions_k, scores_k] = predictFcn_k(predictors_k);
        % Store predictions in the original order
        trained_classifier.ValidationPredictions(cvp.test(k), :) = strtrim(predictions_k);
        trained_classifier.ValidationScores(cvp.test(k),:) = scores_k;
    end
    correct_predictions = strcmp(trained_classifier.ValidationPredictions,strtrim(response));
    trained_classifier.ValidationAccuracy = sum(correct_predictions)/numel(correct_predictions);
    % Show results
    if options.verbose
        fprintf('Model: %s\n  Acc: %5.2f\n',options.model_type,trained_classifier.ValidationAccuracy*100)
    end
end

end

function [classification_model, predict_fcn] = get_model(model_type,predictors,response)
class_names = unique(response);
switch model_type
    case 'tree_fine'
        classification_model = fitctree(...
            predictors, ...
            response, ...
            'SplitCriterion', 'gdi', ...
            'MaxNumSplits', 100, ...
            'Surrogate', 'off', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'tree_medium'
        classification_model = fitctree(...
            predictors, ...
            response, ...
            'SplitCriterion', 'gdi', ...
            'MaxNumSplits', 20, ...
            'Surrogate', 'off', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'tree_coarse'
        classification_model = fitctree(...
            predictors, ...
            response, ...
            'SplitCriterion', 'gdi', ...
            'MaxNumSplits', 4, ...
            'Surrogate', 'off', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'discriminant_linear'
        classification_model = fitcdiscr(...
            predictors, ...
            response, ...
            'DiscrimType', 'linear', ...
            'Gamma', 0, ...
            'FillCoeffs', 'off', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'discriminant_quadratic'
        classification_model = fitcdiscr(...
            predictors, ...
            response, ...
            'DiscrimType', 'quadratic', ...
            'FillCoeffs', 'off', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'logistic_regression_binary_glm'
        if length(class_names) > 2
            ME = MException('SNAP:too_many_classes_binary_glm', ...
                sprintf('Too many classes (%d); incompatible with binary glm',numel(class_names)));
            throw(ME)
        else
            % For logistic regression, the response values must be converted to zeros
            % and ones because the responses are assumed to follow a binomial
            % distribution.
            % 1 or true = 'successful' class
            % 0 or false = 'failure' class
            % NaN - missing response.
            successClass = class_names{1};
            failureClass = class_names{2};
            % Compute the majority response class. If there is a NaN-prediction from
            % fitglm, convert NaN to this majority class label.
            numSuccess = sum(strcmp(strtrim(response), successClass));
            numFailure = sum(strcmp(strtrim(response), failureClass));
            if numSuccess > numFailure
                missingClass = successClass;
            else
                missingClass = failureClass;
            end
            successFailureAndMissingClasses = {successClass; failureClass; missingClass};
            isMissing = cellfun(@(x) isempty(strtrim(x)), response);
            zeroOneResponse = double(strcmp(strtrim(response), successClass));
            zeroOneResponse(isMissing) = NaN;
            % Prepare input arguments to fitglm.
            concatenatedPredictorsAndResponse = [predictors, table(zeroOneResponse)];
            % Train using fitglm.
            classification_model = fitglm(...
                concatenatedPredictorsAndResponse, ...
                'Distribution', 'binomial', ...
                'link', 'logit');
            % Convert predicted probabilities to predicted class labels and scores.
            convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
            returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
            scoresFcn = @(p) [p, 1-p];
            predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );
            predict_fcn = @(x) predictionsAndScoresFcn( predict(classification_model, x) );
        end
    case 'logistic_regression_efficient'
        template = templateLinear(...
            'Learner', 'Logistic', ...
            'Lambda', 'auto', ...
            'BetaTolerance', 0.0001);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_efficient_linear'
        if numel(class_names)>2
            template = templateLinear(...
                'Learner', 'SVM', ...
                'Lambda', 'auto', ...
                'BetaTolerance', 0.0001);
            classification_model = fitcecoc(...
                predictors, ...
                response, ...
                'Learners', template, ...
                'Coding', 'onevsone', ...
                'ClassNames', class_names);
        else
            classification_model = fitclinear(...
                predictors, ...
                response, ...
                'Learner', 'Logistic', ...
                'Lambda', 'auto', ...
                'BetaTolerance', 0.0001, ...
                'ClassNames', class_names);
        end
        predict_fcn = @(x) predict(classification_model, x);
    case 'naive_bayes_gaussian'
        % Expand the Distribution Names per predictor
        % Numerical predictors are assigned either Gaussian or Kernel distribution
        % Gaussian is replaced with Normal when passing to the fitcnb function
        distributionNames =  repmat({'Normal'}, 1, size(predictors,2));
        if any(strcmp(distributionNames,'Kernel'))
            classification_model = fitcnb(...
                predictors, ...
                response, ...
                'Kernel', 'Normal', ...
                'Support', 'Unbounded', ...
                'Standardize', false, ...
                'DistributionNames', distributionNames, ...
                'ClassNames', class_names);
        else
            classification_model = fitcnb(...
                predictors, ...
                response, ...
                'DistributionNames', distributionNames, ...
                'ClassNames', class_names);
        end
        predict_fcn = @(x) predict(classification_model, x);
    case 'naive_bayes_kernel'
        distributionNames =  repmat({'Kernel'}, 1, size(predictors,2));
        if any(strcmp(distributionNames,'Kernel'))
            classification_model = fitcnb(...
                predictors, ...
                response, ...
                'Kernel', 'Normal', ...
                'Support', 'Unbounded', ...
                'Standardize', true, ...
                'DistributionNames', distributionNames, ...
                'ClassNames', class_names);
        else
            classification_model = fitcnb(...
                predictors, ...
                response, ...
                'DistributionNames', distributionNames, ...
                'ClassNames', class_names);
        end
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_linear'
        template = templateSVM(...
            'KernelFunction', 'linear', ...
            'PolynomialOrder', [], ...
            'KernelScale', 'auto', ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_quadratic'
        template = templateSVM(...
            'KernelFunction', 'polynomial', ...
            'PolynomialOrder', 2, ...
            'KernelScale', 'auto', ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_cubic'
        template = templateSVM(...
            'KernelFunction', 'polynomial', ...
            'PolynomialOrder', 3, ...
            'KernelScale', 'auto', ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_gaussian_fine'
        template = templateSVM(...
            'KernelFunction', 'gaussian', ...
            'PolynomialOrder', [], ...
            'KernelScale', 0.71, ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_gaussian_medium'
        template = templateSVM(...
            'KernelFunction', 'gaussian', ...
            'PolynomialOrder', [], ...
            'KernelScale', 2.8, ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_gaussian_coarse'
        template = templateSVM(...
            'KernelFunction', 'gaussian', ...
            'PolynomialOrder', [], ...
            'KernelScale', 11, ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_fine'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Euclidean', ...
            'Exponent', [], ...
            'NumNeighbors', 1, ...
            'DistanceWeight', 'Equal', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_medium'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Euclidean', ...
            'Exponent', [], ...
            'NumNeighbors', 10, ...
            'DistanceWeight', 'Equal', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_coarse'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Euclidean', ...
            'Exponent', [], ...
            'NumNeighbors', 100, ...
            'DistanceWeight', 'Equal', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_cosine'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Cosine', ...
            'Exponent', [], ...
            'NumNeighbors', 10, ...
            'DistanceWeight', 'Equal', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_cubic'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Minkowski', ...
            'Exponent', 3, ...
            'NumNeighbors', 10, ...
            'DistanceWeight', 'Equal', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_weighted'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Euclidean', ...
            'Exponent', [], ...
            'NumNeighbors', 10, ...
            'DistanceWeight', 'SquaredInverse', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'ensemble_boosted_trees'
        if numel(class_names) == 2
            method_boost = 'AdaBoostM1';
        else
            method_boost = 'AdaBoostM2';
        end
        template = templateTree(...
            'MaxNumSplits', 20, ...
            'NumVariablesToSample', 'all');
        classification_model = fitcensemble(...
            predictors, ...
            response, ...
            'Method', method_boost, ...
            'NumLearningCycles', 30, ...
            'Learners', template, ...
            'LearnRate', 0.1, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'ensemble_bagged_trees'
        template = templateTree(...
            'MaxNumSplits', 428, ...
            'NumVariablesToSample', 'all');
        classification_model = fitcensemble(...
            predictors, ...
            response, ...
            'Method', 'Bag', ...
            'NumLearningCycles', 30, ...
            'Learners', template, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'ensemble_subspace_discriminant'
        subspaceDimension = max(1, min(4, width(predictors) - 1));
        classification_model = fitcensemble(...
            predictors, ...
            response, ...
            'Method', 'Subspace', ...
            'NumLearningCycles', 30, ...
            'Learners', 'discriminant', ...
            'NPredToSample', subspaceDimension, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'ensemble_subspace_knn'
        subspaceDimension = max(1, min(4, width(predictors) - 1));
        classification_model = fitcensemble(...
            predictors, ...
            response, ...
            'Method', 'Subspace', ...
            'NumLearningCycles', 30, ...
            'Learners', 'knn', ...
            'NPredToSample', subspaceDimension, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'ensemble_rus_boosted_trees'
        template = templateTree(...
            'MaxNumSplits', 20, ...
            'NumVariablesToSample', 'all');
        classification_model = fitcensemble(...
            predictors, ...
            response, ...
            'Method', 'RUSBoost', ...
            'NumLearningCycles', 30, ...
            'Learners', template, ...
            'LearnRate', 0.1, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'NN_narrow'
        classification_model = fitcnet(...
            predictors, ...
            response, ...
            'LayerSizes', 10, ...
            'Activations', 'relu', ...
            'Lambda', 0, ...
            'IterationLimit', 1000, ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'NN_medium'
        classification_model = fitcnet(...
            predictors, ...
            response, ...
            'LayerSizes', 25, ...
            'Activations', 'relu', ...
            'Lambda', 0, ...
            'IterationLimit', 1000, ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'NN_wide'
        classification_model = fitcnet(...
            predictors, ...
            response, ...
            'LayerSizes', 100, ...
            'Activations', 'relu', ...
            'Lambda', 0, ...
            'IterationLimit', 1000, ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'NN_bilayered'
        classification_model = fitcnet(...
            predictors, ...
            response, ...
            'LayerSizes', [10 10], ...
            'Activations', 'relu', ...
            'Lambda', 0, ...
            'IterationLimit', 1000, ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'NN_trilayered'
        classification_model = fitcnet(...
            predictors, ...
            response, ...
            'LayerSizes', [10 10 10], ...
            'Activations', 'relu', ...
            'Lambda', 0, ...
            'IterationLimit', 1000, ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'kernel_svm'
        template = templateKernel(...
            'Learner', 'svm', ...
            'NumExpansionDimensions', 'auto', ...
            'Lambda', 'auto', ...
            'KernelScale', 'auto', ...
            'Standardize', true, ...
            'IterationLimit', 1000);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'kernel_logistic_regression'
        template = templateKernel(...
            'Learner', 'logistic', ...
            'NumExpansionDimensions', 'auto', ...
            'Lambda', 'auto', ...
            'KernelScale', 'auto', ...
            'Standardize', true, ...
            'IterationLimit', 1000);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    otherwise
        ME = MException('SNAP:invalid_classification_model', ...
                'Invalid model type: %s, choose from following valid models:\ntree_fine,tree_medium,tree_coarse,\ndiscriminant_linear,discriminant_quadratic,\nlogistic_regression_binary_glm,\nlogistic_regression_efficient,svm_efficient_linear,\nnaive_bayes_gaussian,naive_bayes_kernel,\nsvm_linear,svm_quadratic,\nsvm_gaussian_fine,svm_gaussian_medium,svm_gaussian_coarse,\nknn_fine,knn_medium,knn_coarse,knn_cosine,knn_cubic,knn_weighted,\nensemble_boosted_trees,ensemble_bagged_trees,ensemble_subspace_discriminant,ensemble_rus_boosted_trees,\nNN_narrow,NN_medium,NN_wide,NN_bilayered,NN_trilayered,\nkernel_svm,kernel_logistic_regression',options.model_type);
        throw(ME)
end
end