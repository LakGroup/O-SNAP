function trained_classifier = run_SNAP_classification(T,options)
arguments
    T table
    options.method_feature_selection string = 'fscmrmr'
    options.num_features_selected double = 10;
    options.model_type string = 'tree_fine'
    options.num_components_explained double = 0.95
    options.compute_validation_accuracy logical = true;
    options.k_folds double = 5;
    options.verbose logical = true;
end
valid_model_types = ["tree_fine","tree_medium","tree_coarse",...
    "discriminant_linear","discriminant_quadratic",...
    "logistic_regression_binary_glm",...
    "logistic_regression_efficient","svm_efficient_linear",...
    "naive_bayes_gaussian","naive_bayes_kernel",...
    "svm_linear","svm_quadratic",...
    "svm_gaussian_fine","svm_gaussian_medium","svm_gaussian_coarse",...
    "knn_fine","knn_medium","knn_coarse","knn_cosine","knn_cubic","knn_weighted",...
    "ensemble_boosted_trees","ensemble_bagged_trees","ensemble_subspace_discriminant","ensemble_rus_boosted_trees",...
    "NN_narrow","NN_medium","NN_wide","NN_bilayered","NN_trilayered",...
    "kernel_svm","kernel_logistic_regression"];
if ~ismember(options.model_type,valid_model_types)
    ME = MException('SNAP:invalid_classification_model', ...
            'Invalid model type: %s, choose from following valid models:\ntree_fine,tree_medium,tree_coarse,\ndiscriminant_linear,discriminant_quadratic,\nlogistic_regression_binary_glm,\nlogistic_regression_efficient,svm_efficient_linear,\nnaive_bayes_gaussian,naive_bayes_kernel,\nsvm_linear,svm_quadratic,\nsvm_gaussian_fine,svm_gaussian_medium,svm_gaussian_coarse,\nknn_fine,knn_medium,knn_coarse,knn_cosine,knn_cubic,knn_weighted,\nensemble_boosted_trees,ensemble_bagged_trees,ensemble_subspace_discriminant,ensemble_rus_boosted_trees,\nNN_narrow,NN_medium,NN_wide,NN_bilayered,NN_trilayered,\nkernel_svm,kernel_logistic_regression',options.model_type);
    throw(ME)
end

% Extract predictors and response
% This code processes the data into the right shape for training the
% model.
% Assumption is that all predictors are numerical
[T_norm,response] = prepare_voronoi_table_data(T,'numeric_only',1);
class_names = unique(response);

% Perform variable selection
[var_ranked, ~] = select_features(T,"method",options.method_feature_selection);
var_selected = var_ranked(1:options.num_features_selected);
T_norm = T_norm(:,var_selected);

% Apply a PCA to the predictor matrix.
% Run PCA on numeric predictors only. Categorical predictors are passed through PCA untouched.
[pca_coefficients, pca_scores, ~, ~, explained, pca_centers] = pca(...
    table2array(T_norm));
% Keep enough components to explain the desired amount of variance.
if options.num_components_explained < 1
    explained_variance_to_keep_as_fraction = 95/100;
    num_components_to_keep = find(cumsum(explained)/sum(explained) >= explained_variance_to_keep_as_fraction, 1);
else
    num_components_to_keep = options.num_components_explained;
end
pca_coefficients = pca_coefficients(:,1:num_components_to_keep);
T_norm = array2table(pca_scores(:,1:num_components_to_keep));

%% Train a classifier
% This code specifies all the classifier options and trains the classifier.
[classification_model, predict_fcn] = get_model(options.model_type,T_norm,response,class_names);

% Create the result struct with predict function
predictor_extraction_fcn = @(t) t(:, predictor_names);
pca_transformation_fcn = @(x) array2table((table2array(varfun(@double, x)) - pca_centers) * pca_coefficients);
trained_classifier.predictFcn = @(x) predict_fcn(pca_transformation_fcn(predictor_extraction_fcn(x)));

% Add additional fields to the result struct
trained_classifier.ModelType = options.model_type;
trained_classifier.RequiredVariables = var_selected;
trained_classifier.PCACenters = pca_centers;
trained_classifier.PCACoefficients = pca_coefficients;
trained_classifier.ClassificationTree = classification_model;
trained_classifier.About = 'This struct is a trained model exported from Classification Learner R2024b.';
trained_classifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  [yfit,scores] = c.predictFcn(T) \nreplace ''c'' with the name of the variable that is this struct, e.g. ''trainedModel''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');

%% Perform cross-validation
if options.compute_validation_accuracy
    cvp = cvpartition(response, 'KFold', options.k_folds);
    % Initialize the predictions to the proper sizes
    validation_predictions = response;
    num_observations = size(T_norm, 1);
    num_classes = length(class_names);
    validation_scores = NaN(num_observations, num_classes);
    for fold = 1:options.k_folds
        training_predictors = T_norm(cvp.training(fold), :);
        training_response = response(cvp.training(fold), :);
        % Apply a PCA to the predictor matrix.
        % Run PCA on numeric predictors only. Categorical predictors are passed through PCA untouched.
        [pca_coefficients, pca_scores, ~, ~, explained, pca_centers] = pca(...
            table2array(training_predictors));
        % Keep enough components to explain the desired amount of variance.
        if options.num_components_explained
            explained_variance_to_keep_as_fraction = 95/100;
            num_components_to_keep = find(cumsum(explained)/sum(explained) >= explained_variance_to_keep_as_fraction, 1);
        else
            num_components_to_keep = options.num_components_explained;
        end
        pca_coefficients = pca_coefficients(:,1:num_components_to_keep);
        training_predictors = array2table(pca_scores(:,1:num_components_to_keep));
        % Train a classifier
        % This code specifies all the classifier options and trains the classifier.
        [~, predict_fcn] = get_model(options.model_type,training_predictors,training_response,class_names);
        % Create the result struct with predict function
        pca_transformation_fcn = @(x) array2table((table2array(varfun(@double, x)) - pca_centers) * pca_coefficients);
        validation_predict_fcn = @(x) predict_fcn(pca_transformation_fcn(x));
        % Compute validation predictions
        validationPredictors = T_norm(cvp.test(fold), :);
        [foldPredictions, foldScores] = validation_predict_fcn(validationPredictors);
        % Store predictions in the original order
        validation_predictions(cvp.test(fold), :) = foldPredictions;
        validation_scores(cvp.test(fold), :) = foldScores;
    end
    % Compute validation accuracy
    correct_predictions = strcmp(strtrim(validation_predictions), strtrim(response));
    is_missing = cellfun(@(x) all(isspace(x)), response, 'UniformOutput', true);
    correct_predictions = correct_predictions(~is_missing);
    validation_accuracy = sum(correct_predictions)/length(correct_predictions);
    trained_classifier.ValidationAccuracy = validation_accuracy;
    % Show results
    if options.verbose
        % fprintf([trained_classifier.HowToPredict '\n'])
        fprintf('Model: %s\n  Acc: %5.2f\n',options.model_type,validation_accuracy*100)
    end
end

end

function [classification_model, predict_fcn] = get_model(model_type,predictors,response,class_names)
switch model_type
    case 'tree_fine'
        classification_model = fitctree(...
            predictors, ...
            response, ...
            'SplitCriterion', 'gdi', ...
            'MaxNumSplits', 100, ...
            'Surrogate', 'off', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'tree_medium'
        classification_model = fitctree(...
            predictors, ...
            response, ...
            'SplitCriterion', 'gdi', ...
            'MaxNumSplits', 20, ...
            'Surrogate', 'off', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'tree_coarse'
        classification_model = fitctree(...
            predictors, ...
            response, ...
            'SplitCriterion', 'gdi', ...
            'MaxNumSplits', 4, ...
            'Surrogate', 'off', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'discriminant_linear'
        classification_model = fitcdiscr(...
            predictors, ...
            response, ...
            'DiscrimType', 'linear', ...
            'Gamma', 0, ...
            'FillCoeffs', 'off', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'discriminant_quadratic'
        classification_model = fitcdiscr(...
            predictors, ...
            response, ...
            'DiscrimType', 'quadratic', ...
            'FillCoeffs', 'off', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'logistic_regression_binary_glm'
        if length(class_names) > 2
            ME = MException('SNAP:too_many_classes_binary_glm', ...
                sprintf('Too many classes (%d); incompatible with binary glm',length(class_names)));
            throw(ME)
        else
            % For logistic regression, the response values must be converted to zeros
            % and ones because the responses are assumed to follow a binomial
            % distribution.
            % 1 or true = 'successful' class
            % 0 or false = 'failure' class
            % NaN - missing response.
            successClass = class_names{1};
            failureClass = class_names{2};
            % Compute the majority response class. If there is a NaN-prediction from
            % fitglm, convert NaN to this majority class label.
            numSuccess = sum(strcmp(strtrim(response), successClass));
            numFailure = sum(strcmp(strtrim(response), failureClass));
            if numSuccess > numFailure
                missingClass = successClass;
            else
                missingClass = failureClass;
            end
            successFailureAndMissingClasses = {successClass; failureClass; missingClass};
            isMissing = cellfun(@(x) isempty(strtrim(x)), response);
            zeroOneResponse = double(strcmp(strtrim(response), successClass));
            zeroOneResponse(isMissing) = NaN;
            % Prepare input arguments to fitglm.
            concatenatedPredictorsAndResponse = [predictors, table(zeroOneResponse)];
            % Train using fitglm.
            classification_model = fitglm(...
                concatenatedPredictorsAndResponse, ...
                'Distribution', 'binomial', ...
                'link', 'logit');
            % Convert predicted probabilities to predicted class labels and scores.
            convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
            returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
            scoresFcn = @(p) [p, 1-p];
            predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );
            predict_fcn = @(x) predictionsAndScoresFcn( predict(classification_model, x) );
        end
    case 'logistic_regression_efficient'
        template = templateLinear(...
            'Learner', 'Logistic', ...
            'Lambda', 'auto', ...
            'BetaTolerance', 0.0001);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_efficient_linear'
        if length(class_names)>2
            template = templateLinear(...
                'Learner', 'SVM', ...
                'Lambda', 'auto', ...
                'BetaTolerance', 0.0001);
            classification_model = fitcecoc(...
                predictors, ...
                response, ...
                'Learners', template, ...
                'Coding', 'onevsone', ...
                'ClassNames', class_names);
        else
            classification_model = fitclinear(...
                predictors, ...
                response, ...
                'Learner', 'Logistic', ...
                'Lambda', 'auto', ...
                'BetaTolerance', 0.0001, ...
                'ClassNames', class_names);
        end
        predict_fcn = @(x) predict(classification_model, x);
    case 'naive_bayes_gaussian'
        % Expand the Distribution Names per predictor
        % Numerical predictors are assigned either Gaussian or Kernel distribution
        % Gaussian is replaced with Normal when passing to the fitcnb function
        distributionNames =  repmat({'Normal'}, 1, size(predictors,2));
        if any(strcmp(distributionNames,'Kernel'))
            classification_model = fitcnb(...
                predictors, ...
                response, ...
                'Kernel', 'Normal', ...
                'Support', 'Unbounded', ...
                'Standardize', false, ...
                'DistributionNames', distributionNames, ...
                'ClassNames', class_names);
        else
            classification_model = fitcnb(...
                predictors, ...
                response, ...
                'DistributionNames', distributionNames, ...
                'ClassNames', class_names);
        end
        predict_fcn = @(x) predict(classification_model, x);
    case 'naive_bayes_kernel'
        distributionNames =  repmat({'Kernel'}, 1, size(predictors,2));
        if any(strcmp(distributionNames,'Kernel'))
            classification_model = fitcnb(...
                predictors, ...
                response, ...
                'Kernel', 'Normal', ...
                'Support', 'Unbounded', ...
                'Standardize', true, ...
                'DistributionNames', distributionNames, ...
                'ClassNames', class_names);
        else
            classification_model = fitcnb(...
                predictors, ...
                response, ...
                'DistributionNames', distributionNames, ...
                'ClassNames', class_names);
        end
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_linear'
        template = templateSVM(...
            'KernelFunction', 'linear', ...
            'PolynomialOrder', [], ...
            'KernelScale', 'auto', ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_quadratic'
        template = templateSVM(...
            'KernelFunction', 'polynomial', ...
            'PolynomialOrder', 2, ...
            'KernelScale', 'auto', ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_cubic'
        template = templateSVM(...
            'KernelFunction', 'polynomial', ...
            'PolynomialOrder', 3, ...
            'KernelScale', 'auto', ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_gaussian_fine'
        template = templateSVM(...
            'KernelFunction', 'gaussian', ...
            'PolynomialOrder', [], ...
            'KernelScale', 0.71, ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_gaussian_medium'
        template = templateSVM(...
            'KernelFunction', 'gaussian', ...
            'PolynomialOrder', [], ...
            'KernelScale', 2.8, ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'svm_gaussian_coarse'
        template = templateSVM(...
            'KernelFunction', 'gaussian', ...
            'PolynomialOrder', [], ...
            'KernelScale', 11, ...
            'BoxConstraint', 1, ...
            'Standardize', true);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_fine'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Euclidean', ...
            'Exponent', [], ...
            'NumNeighbors', 1, ...
            'DistanceWeight', 'Equal', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_medium'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Euclidean', ...
            'Exponent', [], ...
            'NumNeighbors', 10, ...
            'DistanceWeight', 'Equal', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_coarse'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Euclidean', ...
            'Exponent', [], ...
            'NumNeighbors', 100, ...
            'DistanceWeight', 'Equal', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_cosine'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Cosine', ...
            'Exponent', [], ...
            'NumNeighbors', 10, ...
            'DistanceWeight', 'Equal', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_cubic'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Minkowski', ...
            'Exponent', 3, ...
            'NumNeighbors', 10, ...
            'DistanceWeight', 'Equal', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'knn_weighted'
        classification_model = fitcknn(...
            predictors, ...
            response, ...
            'Distance', 'Euclidean', ...
            'Exponent', [], ...
            'NumNeighbors', 10, ...
            'DistanceWeight', 'SquaredInverse', ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'ensemble_boosted_trees'
        if length(class_names) == 2
            method_boost = 'AdaBoostM1';
        else
            method_boost = 'AdaBoostM2';
        end
        template = templateTree(...
            'MaxNumSplits', 20, ...
            'NumVariablesToSample', 'all');
        classification_model = fitcensemble(...
            predictors, ...
            response, ...
            'Method', method_boost, ...
            'NumLearningCycles', 30, ...
            'Learners', template, ...
            'LearnRate', 0.1, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'ensemble_bagged_trees'
        template = templateTree(...
            'MaxNumSplits', 428, ...
            'NumVariablesToSample', 'all');
        classification_model = fitcensemble(...
            predictors, ...
            response, ...
            'Method', 'Bag', ...
            'NumLearningCycles', 30, ...
            'Learners', template, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'ensemble_subspace_discriminant'
        subspaceDimension = max(1, min(4, width(predictors) - 1));
        classification_model = fitcensemble(...
            predictors, ...
            response, ...
            'Method', 'Subspace', ...
            'NumLearningCycles', 30, ...
            'Learners', 'discriminant', ...
            'NPredToSample', subspaceDimension, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'ensemble_subspace_knn'
        subspaceDimension = max(1, min(4, width(predictors) - 1));
        classification_model = fitcensemble(...
            predictors, ...
            response, ...
            'Method', 'Subspace', ...
            'NumLearningCycles', 30, ...
            'Learners', 'knn', ...
            'NPredToSample', subspaceDimension, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'ensemble_rus_boosted_trees'
        template = templateTree(...
            'MaxNumSplits', 20, ...
            'NumVariablesToSample', 'all');
        classification_model = fitcensemble(...
            predictors, ...
            response, ...
            'Method', 'RUSBoost', ...
            'NumLearningCycles', 30, ...
            'Learners', template, ...
            'LearnRate', 0.1, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'NN_narrow'
        classification_model = fitcnet(...
            predictors, ...
            response, ...
            'LayerSizes', 10, ...
            'Activations', 'relu', ...
            'Lambda', 0, ...
            'IterationLimit', 1000, ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'NN_medium'
        classification_model = fitcnet(...
            predictors, ...
            response, ...
            'LayerSizes', 25, ...
            'Activations', 'relu', ...
            'Lambda', 0, ...
            'IterationLimit', 1000, ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'NN_wide'
        classification_model = fitcnet(...
            predictors, ...
            response, ...
            'LayerSizes', 100, ...
            'Activations', 'relu', ...
            'Lambda', 0, ...
            'IterationLimit', 1000, ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'NN_bilayered'
        classification_model = fitcnet(...
            predictors, ...
            response, ...
            'LayerSizes', [10 10], ...
            'Activations', 'relu', ...
            'Lambda', 0, ...
            'IterationLimit', 1000, ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'NN_trilayered'
        classification_model = fitcnet(...
            predictors, ...
            response, ...
            'LayerSizes', [10 10 10], ...
            'Activations', 'relu', ...
            'Lambda', 0, ...
            'IterationLimit', 1000, ...
            'Standardize', true, ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'kernel_svm'
        template = templateKernel(...
            'Learner', 'svm', ...
            'NumExpansionDimensions', 'auto', ...
            'Lambda', 'auto', ...
            'KernelScale', 'auto', ...
            'Standardize', true, ...
            'IterationLimit', 1000);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    case 'kernel_logistic_regression'
        template = templateKernel(...
            'Learner', 'logistic', ...
            'NumExpansionDimensions', 'auto', ...
            'Lambda', 'auto', ...
            'KernelScale', 'auto', ...
            'Standardize', true, ...
            'IterationLimit', 1000);
        classification_model = fitcecoc(...
            predictors, ...
            response, ...
            'Learners', template, ...
            'Coding', 'onevsone', ...
            'ClassNames', class_names);
        predict_fcn = @(x) predict(classification_model, x);
    otherwise
        ME = MException('SNAP:invalid_classification_model', ...
                'Invalid model type: %s, choose from following valid models:\ntree_fine,tree_medium,tree_coarse,\ndiscriminant_linear,discriminant_quadratic,\nlogistic_regression_binary_glm,\nlogistic_regression_efficient,svm_efficient_linear,\nnaive_bayes_gaussian,naive_bayes_kernel,\nsvm_linear,svm_quadratic,\nsvm_gaussian_fine,svm_gaussian_medium,svm_gaussian_coarse,\nknn_fine,knn_medium,knn_coarse,knn_cosine,knn_cubic,knn_weighted,\nensemble_boosted_trees,ensemble_bagged_trees,ensemble_subspace_discriminant,ensemble_rus_boosted_trees,\nNN_narrow,NN_medium,NN_wide,NN_bilayered,NN_trilayered,\nkernel_svm,kernel_logistic_regression',options.model_type);
        throw(ME)
end
end